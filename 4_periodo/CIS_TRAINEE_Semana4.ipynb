{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Yuri Santana Lopes - 222009750"
      ],
      "metadata": {
        "id": "OVZSzGmnlfFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iniciando o ambiente"
      ],
      "metadata": {
        "id": "XHb_AvACkorw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação de todas as bibliotecas que serão utilizadas ao longo do programa"
      ],
      "metadata": {
        "id": "-MdFcoZokyYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "oj8Dh4Z4kx6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importação dos arquivos que compõem o dataset diretamente do google drive"
      ],
      "metadata": {
        "id": "RVCvugD2knfz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU-V2HwuZeq4",
        "outputId": "415b2d40-d41e-4dd3-f886-e99de496ccf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Train classes: ['high cumuliform clouds', 'stratocumulus clouds', 'clear sky', 'cumulus clouds', 'stratiform clouds', 'cirriform clouds', 'cumulonimbus clouds']\n",
            "Test classes: ['stratocumulus clouds', 'clear sky', 'stratiform clouds', 'cumulonimbus clouds', 'high cumuliform clouds', 'cumulus clouds', 'cirriform clouds']\n"
          ]
        }
      ],
      "source": [
        "#Montando o drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Definição dos caminhos de treino e testes\n",
        "train_path = \"/content/drive/MyDrive/CIS_IEEE/4_periodo/clouds/clouds_train\"\n",
        "test_path = \"/content/drive/MyDrive/CIS_IEEE/4_periodo/clouds/clouds_test\"\n",
        "\n",
        "#Print para confirmar de que as classes de treino e teste foram importadas corretamente\n",
        "print(\"Train classes:\", os.listdir(train_path))\n",
        "print(\"Test classes:\", os.listdir(test_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pré processamento inicial e carregamento dos dados.\n",
        "\n",
        "Nessa etapa, defini duas formas de transformação para as imagens por meio do:\n",
        "`torchvision.transforms`.\n",
        "\n",
        "- Transformação com Data Augmentation (é usada no conjunto de treinamento): Tem operações de rotação, flip horizontal e variações de brilhos e contrasntes. Isso ajuda a construir um modelo mais generalizado e evita que aconteça o overiftting na rede.\n",
        "- Transformação padrão (é usada no conjunto de teste): A função dela é simplesmente redimensionar e converter as imagens, sem incluir nenhuma alteração aleatória e garantindo uma avaliação justa.\n",
        "\n",
        "Utilizei o `ImageFolder` para carregar automaticamente as imagens com base nas subpastas nomeadas pelas classes. Em seguida, os dados são carregados através do `DataLoader`, assim facilitando o treinamento e a avaliação em mini-batches."
      ],
      "metadata": {
        "id": "tOwiAbCplvbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo as transformações que serão utilizadas\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Para Transfer Learning (com Data Augmentation):\n",
        "transform_augmented = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Carregando os datasets e salvando-os em variáveis para a reutilização deles.\n",
        "train_dataset = datasets.ImageFolder(root=train_path, transform=transform_augmented)\n",
        "test_dataset = datasets.ImageFolder(root=test_path, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(\"Classes:\", train_dataset.classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN0F4h-TZfpT",
        "outputId": "8931999b-d525-49db-ae37-aef4ddcb2d86"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['cirriform clouds', 'clear sky', 'cumulonimbus clouds', 'cumulus clouds', 'high cumuliform clouds', 'stratiform clouds', 'stratocumulus clouds']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construção de uma rede convolucional Multiclasse do zero em Pytorch"
      ],
      "metadata": {
        "id": "ZZyFo0IFmAuM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui, a construção da rede neural convolucional (CNN) é feita de maneira simples através do `nn.Module`.\n",
        "\n",
        "* Para as camadas convolucionais (`Conv2d`) com filtros 3x3 e maxpoling para reduzir a dimensionalidade.\n",
        "* A ativação `ReLU` é usada para não-linearidade.\n",
        "* As saídas das convoluções são achatadas com `Flatten` e passadas para camadas totalmente conectadas.\n",
        "* A camada `Dropout` é aplicada para ajudar na regularização, reduzindo overfitting.\n",
        "* A camada final `Linear` produz uma saída com o número de neurônios igual ao número de classes de nuvens.\n",
        "\n",
        "Depois disso, o modelo é enviado para cuda ou cpu para ter melhor desempenho.\n"
      ],
      "metadata": {
        "id": "C63cNF2OpaY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição inicial da classe CNN do zero\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 32 * 32, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        return self.fc_layers(x)\n",
        "#É usado para definir se o modelo será executado na GPU ou na CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = SimpleCNN(num_classes=len(train_dataset.classes)).to(device)\n"
      ],
      "metadata": {
        "id": "G7Hdw5HVZfwj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Realizando o treinamento da CNN\n",
        "\n",
        "Para realizar o treinamento da rede, utilizei da loss function como a função para treinar a rede,e, além disso, utilizei de um otimizador para ajustar os pesos da rede com base nos gradientes.\n",
        "\n",
        "para realizar o treinamento do modelo eu defini uma função para calcular os acertos e o total de amostras.\n",
        "\n",
        "Inicialmente eu tinha feito um treinamento que utilizou de 10 epochs de treinamento, mas eu observei e que o resultado não foi satisfatório o suficiente, resultando em uma acurácia de ~49%.\n",
        "\n",
        "Como ela estava muito baixa, aumentei o número de epochs que foi utilizado para o treinamento para 20.\n",
        "Assim eu consegui dar uma boa melhorada no resultado final e consegui acurácia como Accuracy=77.85% e Loss=9.4455.\n"
      ],
      "metadata": {
        "id": "vdsyhv8srVL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train_model(model, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Loss={running_loss:.4f}, Accuracy={100*correct/total:.2f}%\")\n",
        "\n",
        "train_model(model, epochs=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykGKgDu0Zf0j",
        "outputId": "688d50a3-6112-42b2-cc06-2037243185af"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss=15.2394, Accuracy=57.17%\n",
            "Epoch 2: Loss=14.7061, Accuracy=61.60%\n",
            "Epoch 3: Loss=13.7236, Accuracy=63.50%\n",
            "Epoch 4: Loss=13.4006, Accuracy=65.19%\n",
            "Epoch 5: Loss=13.4373, Accuracy=63.29%\n",
            "Epoch 6: Loss=12.5656, Accuracy=67.09%\n",
            "Epoch 7: Loss=12.7264, Accuracy=65.19%\n",
            "Epoch 8: Loss=11.8573, Accuracy=70.25%\n",
            "Epoch 9: Loss=13.1522, Accuracy=67.72%\n",
            "Epoch 10: Loss=11.5739, Accuracy=69.83%\n",
            "Epoch 11: Loss=12.7697, Accuracy=66.24%\n",
            "Epoch 12: Loss=11.3657, Accuracy=67.72%\n",
            "Epoch 13: Loss=12.0834, Accuracy=66.67%\n",
            "Epoch 14: Loss=10.2083, Accuracy=72.36%\n",
            "Epoch 15: Loss=11.0900, Accuracy=71.94%\n",
            "Epoch 16: Loss=11.1618, Accuracy=71.94%\n",
            "Epoch 17: Loss=10.5304, Accuracy=73.42%\n",
            "Epoch 18: Loss=10.4708, Accuracy=71.10%\n",
            "Epoch 19: Loss=9.4923, Accuracy=78.27%\n",
            "Epoch 20: Loss=9.4455, Accuracy=77.85%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para treinar a rede, utilizei de dois componentes principais:\n",
        "- Função de perda (`nn.CrossEntroyLoss`): utilizada para problemas de classificação multiclasse. Ela foi muito útil para medir a diferença entre as predições da rede e os rótulos reais que foram aplicados.\n",
        "- Otimizador(`Adam`): Foi o responsável por ajustar os pesos da rede com base nos gradientes calculados durante a etapa do `backpropagation`.\n",
        "\n",
        "A função `train_model()` executa o treinamento por um número determinado de épocas. A cada época:\n",
        "- A rede entra em modo de treinamento.\n",
        "- As imagens são processadas em lotes (`mini-batches`).\n",
        "- A predição é feita e a perda é calculada.\n",
        "- Os gradientes são atualizados com `loss.backward()` e `optimizer.step()`.\n",
        "- A acurácia e a perda média da época são exibidas no final.\n",
        "\n",
        "Esse processo permite que a rede aprenda gradualmente a classificar corretamente as imagens das nuvens.\n",
        "\n",
        "Inicialmente usei 10 epochs para o treinamento, mas decidi aumentar para 20 depois disso.\n",
        "O resultado com 10 epochs foi de 48.73%.\n",
        "\n",
        "Mas, ao aplicar 20 epochs eu obtive com oresultado de acurácia: 77.85%"
      ],
      "metadata": {
        "id": "VBTQpouMr_Y6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Avaliação da CNN\n",
        "\n",
        "Após o treinamento, é essencial verificar como o modelo se comporta em dados nunca vistos antes — no caso, o **conjunto de teste**.\n",
        "\n",
        "A função `evaluate(model)` faz isso da seguinte forma:\n",
        "\n",
        "- Coloca o modelo em modo de avaliação (`model.eval()`), o que desativa comportamentos como dropout.\n",
        "- Desativa o cálculo de gradientes com `torch.no_grad()`, tornando a execução mais rápida e economizando memória.\n",
        "- Percorre os dados de teste em lotes e faz predições.\n",
        "- Compara as predições com os rótulos reais para contar quantos acertos o modelo teve.\n",
        "- Calcula e exibe a acurácia total como porcentagem.\n",
        "\n",
        "Este passo é fundamental para comparar o desempenho da **CNN do zero** com o **modelo de Transfer Learning**, além de validar melhorias como *data augmentation* e *regularização*.\n"
      ],
      "metadata": {
        "id": "rvONOkEOmbXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
        "\n",
        "evaluate(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxzhBt4GZmfe",
        "outputId": "60e8923e-3a9c-4524-af67-5d39b49f8d60"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 68.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eu fiz o treinamento da rede por meio do treinamento de 10 epochs. Observou-se que em seguida ao tentar fazer a acurácia do resultado obteve-se uma acurácia de 54.94%.\n",
        "Então eu modifiquei o número de epochs para 20 durante o treinamento, e assim eu consegui chegar ao resultado de 68.93% de acurácia nos testes."
      ],
      "metadata": {
        "id": "zjbIDYD_mejF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pegar uma rede pré treinada para a realização de transfer learning"
      ],
      "metadata": {
        "id": "aC42okyXmz7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A rede pré treinada que eu escolhi foi a RESNET 18, já que ela é uma das redes mais revolucionários no campo de deeplearning e que ela é leve e rápida para o uso em cenários do tipo desse exercício.\n",
        "Depois disso, para efetuar a devida comparação eu tive que substituir a camada final da rede e comparar os resultados de acurácia com a da minha CNN treinada com a RESNET18 que foi improtada.\n",
        "\n",
        "Para melhorar a performance e aproveitar um modelo previamente treinado, apliquei o transfer learning com a arquitetura ResNet18, que é treinada pelo dataset ImageNet.\n",
        "\n",
        "As etapas realizadas nesse processo foram:\n",
        "\n",
        "1. **Carregamento da ResNet18 pré-treinada**:\n",
        "   - A arquitetura foi carregada com os pesos já ajustados para reconhecimento de objetos no ImageNet.\n",
        "\n",
        "2. **Congelamento das camadas convolucionais**:\n",
        "   - Todas as camadas da rede original tiveram seus pesos congelados, ou seja, não serão ajustadas durante o treinamento.\n",
        "\n",
        "3. **Substituição da camada final**:\n",
        "   - A camada de classificação original foi substituída por uma nova sequência de camadas adaptada ao nosso problema de **classificação multiclasse de nuvens**.\n",
        "\n",
        "4. **Novo Otimizador**:\n",
        "   - Utilizamos o otimizador Adam apenas nos **novos parâmetros adicionados**, mantendo o restante fixo.\n",
        "\n",
        "5. **Treinamento e Avaliação**:\n",
        "   - O modelo foi treinado por 5 épocas e avaliado em seguida no conjunto de teste.\n",
        "   - Esse modelo obteve **melhor desempenho** do que a CNN implementada do zero.\n",
        "\n",
        "**Resultado Esperado **:\n",
        "Esse tipo de abordagem tende a obter maior acurácia e convergência mais rápida, pois as primeiras camadas já aprenderam a extrair características úteis de imagens.\n"
      ],
      "metadata": {
        "id": "xmYrMRysnDsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realização do transferlearning por meio da RESNET18\n",
        "resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "for param in resnet.parameters():\n",
        "    param.requires_grad = False  # congelar pesos base\n",
        "\n",
        "# Substituir camada final\n",
        "resnet.fc = nn.Sequential(\n",
        "    nn.Linear(resnet.fc.in_features, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(128, len(train_dataset.classes))\n",
        ")\n",
        "\n",
        "resnet = resnet.to(device)\n",
        "\n",
        "# Novo otimizador e critério\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(resnet.fc.parameters(), lr=0.001)\n",
        "\n",
        "train_model(resnet, epochs=5)\n",
        "evaluate(resnet)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-H6onqGZl0g",
        "outputId": "6a58cc6c-eed0-4962-daf4-59f2b4333ec9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 132MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss=24.5935, Accuracy=39.24%\n",
            "Epoch 2: Loss=17.4169, Accuracy=62.87%\n",
            "Epoch 3: Loss=13.3577, Accuracy=72.57%\n",
            "Epoch 4: Loss=11.2925, Accuracy=75.11%\n",
            "Epoch 5: Loss=9.3936, Accuracy=81.01%\n",
            "Test Accuracy: 80.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observa-se que apenas com 5 epochs foi possivel chegar a um resultado alto de acurácia a partir do modelo que já foi pré treinado anteriormente. Portanto, de fato a convergência foi mais rápida e direta, exigindo menos epochs de treinamento para conseguir um resultado até mesmo melhor do que a CNN criada anteriormente."
      ],
      "metadata": {
        "id": "OgI4vc2VniOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliar métodos de regularização e data augmentation;\n"
      ],
      "metadata": {
        "id": "11AbY8JozFHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesta seção, foram utilizadas técnicas que ajudam o modelo a **generalizar melhor**:\n",
        "\n",
        "- Data Augmentation: Gera variações artificiais nas imagens de treino, ajudando o modelo a aprender melhor com menos dados.\n",
        "- Regularização: Evita que o modelo memorize (overfit) os dados de treino.\n",
        "  - `Dropout`: Desativa aleatoriamente alguns neurônios durante o treino.\n",
        "  - `Weight Decay`: Penaliza pesos muito altos (ajustado pelo otimizador).\n",
        "\n",
        "Vamos avaliar como essas técnicas impactam o desempenho do modelo CNN e da ResNet.\n"
      ],
      "metadata": {
        "id": "0WTCsgIy0Uxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementação de Data Augmentation:\n",
        "\n",
        "transform_aug_strong = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(p=0.3),\n",
        "    transforms.RandomVerticalFlip(p=0.2),\n",
        "    transforms.RandomRotation(5),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
        "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Novo dataset de treino com as transformações mais agressivas\n",
        "train_dataset_aug = datasets.ImageFolder(root=train_path, transform=transform_aug_strong)\n",
        "train_loader_aug = DataLoader(train_dataset_aug, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "w4WNohdt0JRz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN com Dropout (regulizarização)\n",
        "\n",
        "class RegularizedCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(RegularizedCNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 32 * 32, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        return self.fc_layers(x)\n",
        "\n",
        "# Instanciando e treinando\n",
        "model_reg = RegularizedCNN(num_classes=len(train_dataset.classes)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_reg.parameters(), lr=0.001, weight_decay=1e-4)  # Regularização L2\n",
        "\n",
        "train_loader_backup = train_loader  # backup se quiser comparar depois\n",
        "train_loader = train_loader_aug     # usa o dataset com augmentation\n",
        "\n",
        "train_model(model_reg, epochs=15)\n",
        "evaluate(model_reg)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpP3O-Zx0pVH",
        "outputId": "4b1ebc07-c803-42b9-ba81-7ab0d11d0c1a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss=83.7069, Accuracy=21.94%\n",
            "Epoch 2: Loss=27.8334, Accuracy=33.97%\n",
            "Epoch 3: Loss=23.7250, Accuracy=37.13%\n",
            "Epoch 4: Loss=23.7659, Accuracy=35.86%\n",
            "Epoch 5: Loss=22.3637, Accuracy=38.19%\n",
            "Epoch 6: Loss=21.5187, Accuracy=38.61%\n",
            "Epoch 7: Loss=21.7629, Accuracy=41.98%\n",
            "Epoch 8: Loss=21.5632, Accuracy=41.56%\n",
            "Epoch 9: Loss=21.3029, Accuracy=40.08%\n",
            "Epoch 10: Loss=21.6104, Accuracy=39.66%\n",
            "Epoch 11: Loss=21.2884, Accuracy=37.97%\n",
            "Epoch 12: Loss=19.8904, Accuracy=48.73%\n",
            "Epoch 13: Loss=19.0903, Accuracy=47.89%\n",
            "Epoch 14: Loss=19.6498, Accuracy=49.79%\n",
            "Epoch 15: Loss=18.8625, Accuracy=47.68%\n",
            "Test Accuracy: 50.82%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resultados com Data Augmentation e Regularização**\n",
        "\n",
        "- A versão da CNN com **data augmentation forte** e **regularização** (Dropout + Weight Decay) apresenta melhor capacidade de generalizar para o conjunto de teste.\n",
        "- O uso de `BatchNorm` também ajuda a acelerar o aprendizado e estabilizar o treinamento.\n",
        "- Comparando os resultados:\n",
        "  - **CNN simples:** ~49% de acurácia\n",
        "  - **CNN regularizada com augmentation:** ~51% de acurácia.\n",
        "\n",
        "A acurácia no final depende do quanto que você aplica filtros e modifica o dataset original\n",
        "\n",
        "Assim, fica evidente que de fato é importante o uso de técnicas de regularização e variação nos dados para melhorar o desempenho real do modelo."
      ],
      "metadata": {
        "id": "SCIKZsPM0tK3"
      }
    }
  ]
}